[
  {
    "name": "softmax",
    "arguments": [
      {
        "name": "x",
        "default": null
      },
      {
        "name": "axis",
        "default": -1
      }
    ],
    "docstring": "Softmax converts a real vector to a vector of categorical probabilities.\n\n  The elements of the output vector are in range (0, 1) and sum to 1.\n\n  Each vector is handled independently. The `axis` argument sets which axis\n  of the input the function is applied along.\n\n  Softmax is often used as the activation for the last\n  layer of a classification network because the result could be interpreted as\n  a probability distribution.\n\n  The softmax of each vector x is calculated by `exp(x)/tf.reduce_sum(exp(x))`.\n  The input values in are the log-odds of the resulting probability.\n\n  Arguments:\n      x : Input tensor.\n      axis: Integer, axis along which the softmax normalization is applied.\n\n  Returns:\n      Tensor, output of softmax transformation (all values are non-negative\n        and sum to 1).\n\n  Raises:\n      ValueError: In case `dim(x) == 1`.\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "elu",
    "arguments": [
      {
        "name": "x",
        "default": null
      },
      {
        "name": "alpha",
        "default": 1
      }
    ],
    "docstring": "Exponential linear unit.\n\n  Arguments:\n      x: Input tensor.\n      alpha: A scalar, slope of negative section.\n\n  Returns:\n      The exponential linear activation: `x` if `x > 0` and\n        `alpha * (exp(x)-1)` if `x < 0`.\n\n  Reference:\n      - [Fast and Accurate Deep Network Learning by Exponential\n        Linear Units (ELUs)](https://arxiv.org/abs/1511.07289)\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "selu",
    "arguments": [
      {
        "name": "x",
        "default": null
      }
    ],
    "docstring": "Scaled Exponential Linear Unit (SELU).\n\n  The Scaled Exponential Linear Unit (SELU) activation function is:\n  `scale * x` if `x > 0` and `scale * alpha * (exp(x) - 1)` if `x < 0`\n  where `alpha` and `scale` are pre-defined constants\n  (`alpha = 1.67326324`\n  and `scale = 1.05070098`).\n  The SELU activation function multiplies  `scale` > 1 with the\n  `[elu](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/elu)`\n  (Exponential Linear Unit (ELU)) to ensure a slope larger than one\n  for positive net inputs.\n\n  The values of `alpha` and `scale` are\n  chosen so that the mean and variance of the inputs are preserved\n  between two consecutive layers as long as the weights are initialized\n  correctly (see [`lecun_normal` initialization]\n  (https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal))\n  and the number of inputs is \"large enough\"\n  (see references for more information).\n\n  ![]https://cdn-images-1.medium.com/max/1600/1*m0e8lZU_Zrkh4ESfQkY2Pw.png\n  (Courtesy: Blog on Towards DataScience at\n  https://towardsdatascience.com/selu-make-fnns-great-again-snn-8d61526802a9)\n\n  Example Usage:\n\n  >>> n_classes = 10  #10-class problem\n  >>> from tensorflow.python.keras.layers import Dense\n  >>> model = tf.keras.Sequential()\n  >>> model.add(Dense(64, kernel_initializer='lecun_normal',\n  ...                 activation='selu', input_shape=(28, 28, 1)))\n  >>> model.add(Dense(32, kernel_initializer='lecun_normal',\n  ...                 activation='selu'))\n  >>> model.add(Dense(16, kernel_initializer='lecun_normal',\n  ...                 activation='selu'))\n  >>> model.add(Dense(n_classes, activation='softmax'))\n\n  Arguments:\n      x: A tensor or variable to compute the activation function for.\n\n  Returns:\n      The scaled exponential unit activation: `scale * elu(x, alpha)`.\n\n  # Note\n      - To be used together with the initialization \"[lecun_normal]\n      (https://www.tensorflow.org/api_docs/python/tf/keras/initializers/lecun_normal)\".\n      - To be used together with the dropout variant \"[AlphaDropout]\n      (https://www.tensorflow.org/api_docs/python/tf/keras/layers/AlphaDropout)\".\n\n  References:\n      [Self-Normalizing Neural Networks (Klambauer et al, 2017)]\n      (https://arxiv.org/abs/1706.02515)\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "softplus",
    "arguments": [
      {
        "name": "x",
        "default": null
      }
    ],
    "docstring": "Softplus activation function.\n\n  Arguments:\n      x: Input tensor.\n\n  Returns:\n      The softplus activation: `log(exp(x) + 1)`.\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "softsign",
    "arguments": [
      {
        "name": "x",
        "default": null
      }
    ],
    "docstring": "Softsign activation function.\n\n  Arguments:\n      x: Input tensor.\n\n  Returns:\n      The softsign activation: `x / (abs(x) + 1)`.\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "swish",
    "arguments": [
      {
        "name": "x",
        "default": null
      }
    ],
    "docstring": "Swish activation function.\n\n  Arguments:\n      x: Input tensor.\n\n  Returns:\n      The swish activation applied to `x`.\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "relu",
    "arguments": [
      {
        "name": "x",
        "default": null
      },
      {
        "name": "alpha",
        "default": 0
      },
      {
        "name": "max_value",
        "default": "None"
      },
      {
        "name": "threshold",
        "default": 0
      }
    ],
    "docstring": "Applies the rectified linear unit activation function.\n\n  With default values, this returns the standard ReLU activation:\n  `max(x, 0)`, the element-wise maximum of 0 and the input tensor.\n\n  Modifying default parameters allows you to use non-zero thresholds,\n  change the max value of the activation,\n  and to use a non-zero multiple of the input for values below the threshold.\n\n  For example:\n\n  >>> foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32)\n  >>> tf.keras.activations.relu(foo).numpy()\n  array([ 0.,  0.,  0.,  5., 10.], dtype=float32)\n  >>> tf.keras.activations.relu(foo, alpha=0.5).numpy()\n  array([-5. , -2.5,  0. ,  5. , 10. ], dtype=float32)\n  >>> tf.keras.activations.relu(foo, max_value=5).numpy()\n  array([0., 0., 0., 5., 5.], dtype=float32)\n  >>> tf.keras.activations.relu(foo, threshold=5).numpy()\n  array([-0., -0.,  0.,  0., 10.], dtype=float32)\n\n  Arguments:\n      x: Input `tensor` or `variable`.\n      alpha: A `float` that governs the slope for values lower than the\n        threshold.\n      max_value: A `float` that sets the saturation threshold (the largest value\n        the function will return).\n      threshold: A `float` giving the threshold value of the activation function\n        below which values will be damped or set to zero.\n\n  Returns:\n      A `Tensor` representing the input tensor,\n      transformed by the relu activation function.\n      Tensor will be of the same shape and dtype of input `x`.\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "tanh",
    "arguments": [
      {
        "name": "x",
        "default": null
      }
    ],
    "docstring": "Hyperbolic tangent activation function.\n\n  For example:\n\n  >>> a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n  >>> b = tf.keras.activations.tanh(a)\n  >>> b.numpy()\n  array([-0.9950547, -0.7615942,  0.       ,  0.7615942,  0.9950547],\n          dtype=float32)\n\n  Arguments:\n      x: Input tensor.\n\n  Returns:\n      Tensor of same shape and dtype of input `x`, with tanh activation:\n      `tanh(x) = sinh(x)/cosh(x) = ((exp(x) - exp(-x))/(exp(x) + exp(-x)))`.\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "sigmoid",
    "arguments": [
      {
        "name": "x",
        "default": null
      }
    ],
    "docstring": "Sigmoid activation function.\n\n  Applies the sigmoid activation function. The sigmoid function is defined as\n  1 divided by (1 + exp(-x)). It's curve is like an \"S\" and is like a smoothed\n  version of the Heaviside (Unit Step Function) function. For small values\n  (<-5) the sigmoid returns a value close to zero and for larger values (>5)\n  the result of the function gets close to 1.\n\n  Sigmoid is equivalent to a 2-element Softmax, where the second element is\n  assumed to be zero.\n\n  For example:\n\n  >>> a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)\n  >>> b = tf.keras.activations.sigmoid(a)\n  >>> b.numpy() >= 0.0\n  array([ True,  True,  True,  True,  True])\n\n  Arguments:\n      x: Input tensor.\n\n  Returns:\n      Tensor with the sigmoid activation: `(1.0 / (1.0 + exp(-x)))`.\n      Tensor will be of same shape and dtype of input `x`.\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "exponential",
    "arguments": [
      {
        "name": "x",
        "default": null
      }
    ],
    "docstring": "Exponential activation function.\n\n  For example:\n\n  >>> a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n  >>> b = tf.keras.activations.exponential(a)\n  >>> b.numpy()\n  array([ 0.04978707,  0.36787945,  1.        ,  2.7182817 , 20.085537  ],\n        dtype=float32)\n\n  Arguments:\n      x: Input tensor.\n\n  Returns:\n      Tensor with exponential activation: `exp(x)`. Tensor will be of same\n      shape and dtype of input `x`.\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "hard_sigmoid",
    "arguments": [
      {
        "name": "x",
        "default": null
      }
    ],
    "docstring": "Hard sigmoid activation function.\n\n  Faster to compute than sigmoid activation.\n\n  For example:\n\n  >>> a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n  >>> b = tf.keras.activations.hard_sigmoid(a)\n  >>> b.numpy()\n  array([0. , 0.3, 0.5, 0.7, 1. ], dtype=float32)\n\n  Arguments:\n      x: Input tensor.\n\n  Returns:\n    The hard sigmoid activation:\n\n      - `0` if `x < -2.5`\n      - `1` if `x > 2.5`\n      - `0.2 * x + 0.5` if `-2.5 <= x <= 2.5`.\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  },
  {
    "name": "linear",
    "arguments": [
      {
        "name": "x",
        "default": null
      }
    ],
    "docstring": "Linear activation function.\n\n  For example:\n\n  >>> a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)\n  >>> b = tf.keras.activations.linear(a)\n  >>> b.numpy()\n  array([-3., -1.,  0.,  1.,  3.], dtype=float32)\n\n  Arguments:\n      x: Input tensor.\n\n  Returns:\n      the input unmodified.\n  ",
    "aliases": [],
    "file": "keras/activations.py"
  }
]